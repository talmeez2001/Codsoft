{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f537621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Aafra Md. Hanif Shaikh\n",
    "# Task Name: CUSTOMER_CHURN_PREDICTION\n",
    "# 3st task in the list of tasks\n",
    "# Task Category: Machine Learning\n",
    "# Date Of Submission: \n",
    "# Linkedin Profile: \n",
    "# Github Repository: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e72961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a098ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace 'your_data.csv' with your dataset's filename)\n",
    "data = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6244619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_boosting_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# 1. Handle missing values\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# 2. Feature selection and target variable\n",
    "X = data.drop(columns=['Exited', 'RowNumber', 'CustomerId', 'Surname'])  # Features (exclude 'Exited', non-numeric, and non-ordinal categorical columns)\n",
    "y = data['Exited']  # Target variable ('Exited')\n",
    "\n",
    "# 3. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Standardize the numeric features\n",
    "numeric_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# 5. One-hot encoding for categorical columns\n",
    "categorical_columns = ['Geography', 'Gender']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_columns]))\n",
    "X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_columns]))\n",
    "\n",
    "X_train_encoded.index = X_train.index\n",
    "X_test_encoded.index = X_test.index\n",
    "\n",
    "X_train.drop(categorical_columns, axis=1, inplace=True)\n",
    "X_test.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_encoded], axis=1)\n",
    "\n",
    "# Convert column names to strings\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "# Model Selection and Training\n",
    "# You can choose between Logistic Regression, Random Forest, or Gradient Boosting\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Save the Logistic Regression model to a file\n",
    "joblib.dump(lr_model, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Save the Random Forest model to a file\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Save the Gradient Boosting model to a file\n",
    "joblib.dump(gb_model, 'gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e508d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.811\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[1543   64]\n",
      " [ 314   79]]\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89      1607\n",
      "           1       0.55      0.20      0.29       393\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.69      0.58      0.59      2000\n",
      "weighted avg       0.78      0.81      0.77      2000\n",
      "\n",
      "Accuracy for Random Forest: 0.8695\n",
      "Confusion Matrix for Random Forest:\n",
      "[[1547   60]\n",
      " [ 201  192]]\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92      1607\n",
      "           1       0.76      0.49      0.60       393\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.82      0.73      0.76      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n",
      "Accuracy for Gradient Boosting: 0.8675\n",
      "Confusion Matrix for Gradient Boosting:\n",
      "[[1543   64]\n",
      " [ 201  192]]\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      1607\n",
      "           1       0.75      0.49      0.59       393\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.82      0.72      0.76      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "def evaluate_model(model, model_name):\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy for {model_name}: {accuracy}\")\n",
    "    print(f\"Confusion Matrix for {model_name}:\\n{confusion_matrix(y_test, predictions)}\")\n",
    "    print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, predictions)}\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "evaluate_model(lr_model, 'Logistic Regression')\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(rf_model, 'Random Forest')\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "evaluate_model(gb_model, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118db2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
